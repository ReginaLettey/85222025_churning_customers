# -*- coding: utf-8 -*-
"""85222025_Churning_Customers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15XS0m4gdZKBZE9WJXLhdA8fJbRXw3yi_

**Developing a churn prediction model that assists telecom operators in predicting customers who are most likely subject to churn**.
"""

from google.colab import drive
drive.mount('/content/drive')

"""IMPORTING LIBRARIES"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import GridSearchCV
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

!pip install keras-tuner
import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras.layers import Input, Dense
from keras import models
from keras import Model
from tensorflow.keras import layers
import kerastuner as kt
from kerastuner.tuners import RandomSearch
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, roc_auc_score

"""**FEATURE EXTRACTION**"""

CustomerChurn_dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CustomerChurn_dataset.csv')

CustomerChurn_dataset

# Drop customerID column
CustomerChurn_dataset = CustomerChurn_dataset.drop('customerID', axis=1)

CustomerChurn_dataset

CustomerChurn_dataset.head()

CustomerChurn_dataset.info()

CustomerChurn_dataset['TotalCharges'] = pd.to_numeric(CustomerChurn_dataset['TotalCharges'], errors='coerce')
CustomerChurn_dataset.info()

##show the number of rows and columns
CustomerChurn_dataset.shape

##checking if any attributes in the dataset has NaNs
CustomerChurn_dataset.isnull().sum()

# impute NaNs in 'TotalCharges' with the mean value
CustomerChurn_dataset['TotalCharges'].fillna(CustomerChurn_dataset['TotalCharges'].mean(), inplace=True)

## checking if there are still NaNs in any attribute
null = CustomerChurn_dataset.isnull().sum()
null

# Identifying the columns with non-numeric or categorical values
Cat_values = CustomerChurn_dataset.select_dtypes(include=['object', 'category']).columns
Cat_values

#Encode the categorical columns

for column in Cat_values:
  CustomerChurn_dataset[column], _ = pd.factorize(CustomerChurn_dataset[column])
  encoded_cols = CustomerChurn_dataset[Cat_values]
  encoded_cols

#combine/concatenate numerical columns to the encoded data
numerical_values = ['TotalCharges', 'MonthlyCharges']
new_data_set = pd.concat([ CustomerChurn_dataset[Cat_values],  CustomerChurn_dataset[numerical_values]], axis=1)
new_data_set

df = pd.DataFrame(CustomerChurn_dataset)

# Calculate correlation matrix
correlation_matrix = df.corr()

correlation_matrix

# Select columns with correlation greater than 0.5
highly_correlated_columns = [column for column in correlation_matrix.columns if any(correlation_matrix[column] > 0.5)]
highly_correlated_columns

##Remove or drop churn since it is the dependent variable
highly_correlated_columns.remove('Churn')
highly_correlated_columns

"""**EXPLORATORY DATA ANALYSIS**"""

## Visualizing the distribution of Churn against all other highly correlated columns


# Set the style of seaborn
sns.set(style="whitegrid")

# Create a figure with subplots
fig, axes = plt.subplots(nrows=5, ncols=4, figsize=(20, 20))

# Flatten the axes for easy iteration
axes = axes.flatten()

# Specify the features to visualize
features = ['gender', 'SeniorCitizen', 'Partner', 'Dependents',
    'tenure', 'PhoneService', 'MultipleLines', 'InternetService',
    'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',
    'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',
    'PaymentMethod', 'MonthlyCharges', 'TotalCharges']

# Loop through each feature and plot
for i, feature in enumerate(features):
    sns.countplot(x=feature, hue='Churn', data=CustomerChurn_dataset, ax=axes[i])

# Adjust layout
plt.tight_layout()
plt.show()

"""The graphs above represent the rate at which customers churn with respect to each attribute. 0 represent the count or proportion of customers who have not churned.
1 represent the count or proportion of customers who have churned.
"""

# Splitting the dataset into input and output
X = CustomerChurn_dataset.drop('Churn', axis=1) # Input features (all columns except 'Churn')
y = CustomerChurn_dataset['Churn']# Target variable

#from sklearn.preprocessing import StandardScaler

# Assuming X is a DataFrame with both numeric and non-numeric columns
# Select only numeric columns for scaling
numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns
X_numeric = X[numeric_cols]

# Initialize the scaler and fit_transform on the training data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_numeric)

# Replace the scaled values back into the original DataFrame
X[numeric_cols] = X_train
X

# Split the dataset into a training set (80%) and a test set (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)

"""**MULTI-LAYER PERCEPTRON MODEL USING FUNCTIONAL API**"""

!pip install keras
!pip install tensorflow
!pip install tensorflow
!pip install scikeras
!pip install --upgrade tensorflow
#!pip install keras==2.9
!pip install tensorflow-addons
!pip install keras==2.15.0

import tensorflow as tf
from tensorflow.keras.models import Model
from scikeras.wrappers import KerasClassifier
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split


def create_keras_model(optimizer = 'adam', hidden_layer_1_unit = 32, hidden_layer_2_unit = 16):
    input_layer = Input(shape=19,)
    # Define hidden layers with the specified number of units (neurons) and activation functions
    hidden_layer_1 = Dense(hidden_layer_1_unit, activation='relu')(input_layer)
    hidden_layer_2 = Dense(hidden_layer_2_unit, activation='relu')(hidden_layer_1)

    output_layer = Dense(1, activation='sigmoid')(hidden_layer_2)

    model = Model(inputs=input_layer, outputs=output_layer)

    # Compile the model
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Create a KerasClassifier based on your function
keras_model = KerasClassifier(model=create_keras_model,epochs=10, batch_size=32, verbose=0, hidden_layer_1_unit = 32, hidden_layer_2_unit = 16)

# Define hyperparameters for grid search
param_grid = {
    'optimizer': ['adam', 'sgd', 'rmsprop'],
    'hidden_layer_1_unit': [32, 64, 128],
    'hidden_layer_2_unit': [16, 32, 64],
}

"""USING GRIDSEARCH TO FIND THE OPTIMAL PARAMETERS"""

# Use GridSearchCV to find the optimal hyperparameters
grid = GridSearchCV(estimator=keras_model, param_grid=param_grid, scoring='roc_auc', cv=3, error_score='raise')
grid_result = grid.fit(X_train, y_train)

# Get the best parameters
best_params = grid_result.best_params_

# Display the results
print(f"Best Parameters: {best_params}")

"""GETTING ACCURACY AND AUC SCORE"""

from sklearn.metrics import accuracy_score, roc_auc_score

# Get the best parameters
best_params = grid_result.best_params_

# Create a new KerasClassifier with the best parameters
best_keras_model = KerasClassifier(model=create_keras_model, **best_params, epochs=10, batch_size=32, verbose=0)

# Train the model with the best parameters
best_keras_model.fit(X_train, y_train)

# Predict on the test set
y_pred = best_keras_model.predict(X_test)

# Evaluate accuracy and AUC score
accuracy = accuracy_score(y_test, y_pred)
auc_score = roc_auc_score(y_test, y_pred)

# Display the results
print(f"Accuracy: {accuracy}")
print(f"AUC Score: {auc_score}")

"""OPTIMIZING WITH THE BEST PARAMETERS TO GET THE FINAL ACCURACY AND AUC SCORES"""

from sklearn.metrics import accuracy_score, roc_auc_score
from tensorflow.keras.optimizers import SGD

input_layer = Input(shape=19,)
# Define hidden layers with the specified number of units (neurons) and activation functions
hidden_layer_1 = Dense(64, activation='relu')(input_layer)
hidden_layer_2 = Dense(32, activation='relu')(hidden_layer_1)
output_layer = Dense(1, activation='sigmoid')(hidden_layer_2)

Opt_model = Model(inputs=input_layer, outputs=output_layer)

    # Compile the model
Opt_model.compile(optimizer= SGD(), loss='binary_crossentropy', metrics=['accuracy'])
# Train the model with the best parameters
Opt_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

# Predict on the test set
y_pred_optimized = best_keras_model.predict(X_test)
y_pred_opt_binary=(y_pred > 0.5).astype(int)

# Evaluate accuracy and AUC score
accuracy = accuracy_score(y_test,  y_pred_opt_binary)
auc_score = roc_auc_score(y_test, y_pred_optimized)

# Display the results
print(f"Accuracy: {accuracy}")
print(f"AUC Score: {auc_score}")

"""**SAVING THE MODEL**"""

import pickle
model = Opt_model
with open("model.pkl", "wb") as file:
    pickle.dump(model, file)
with open('scaler.pkl', 'wb') as scaler_file:
  pickle.dump(scaler, scaler_file)